{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Copy of Ensemble_Learning_with_Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnO9ucOvyVYL",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble Learning in Python\n",
        "In this tutorial, you'll learn what ensemble is and how it improves the performance of a machine learning model.\n",
        "You all know that the field of machine learning keeps getting better and better with time. Predictive models form the core of machine learning. Better the accuracy better the model is and so is the solution to a particular problem. In this post, you are going to learn about something called Ensemble learning which is a potent technique to improve the performance of your machine learning model.\n",
        "\n",
        "In this post you will cover:\n",
        "\n",
        "- What is Ensemble learning?\n",
        "- How it improves the performance of a machine learning model?\n",
        "- Different Ensemble learning methods\n",
        "- Pitfalls of Ensembles\n",
        "- A Pythonic implementation of different Ensemble learning methods with a real test dataset\n",
        "- Further studies on Ensemble learning\n",
        "\n",
        "So, let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZC0k82WyVYM",
        "colab_type": "text"
      },
      "source": [
        "## What is Ensemble learning?\n",
        "In the world of Statistics and Machine Learning, Ensemble learning techniques attempt to make the performance of the predictive models better by improving their accuracy. Ensemble Learning is a process using which multiple machine learning models (such as classifiers) are strategically constructed to solve a particular problem. \n",
        "\n",
        "Let's take a real example to build the intuition.\n",
        "\n",
        "Suppose, you want to invest in a company XYZ. You are not sure about its performance though. So, you look for advice on whether the stock price will increase by more than 6% per annum or not? You decide to approach various experts having diverse domain experience:\n",
        "\n",
        "- **Employee of Company XYZ:** This person knows the internal functionality of the company and has the insider information about the functionality of the firm. But he lacks a broader perspective on how are competitors innovating, how is the technology evolving and what will be the impact of this evolution on Company XYZ’s product. In the past, he has been right `70%` times.\n",
        "\n",
        "- **Financial Advisor of Company XYZ:** This person has a broader perspective on how companies strategy will fair in this competitive environment. However, he lacks a view on how the company’s internal policies are fairing off. In the past, he has been right 75% times.\n",
        "\n",
        "- **Stock Market Trader:** This person has observed the company’s stock price over the past 3 years. He knows the seasonality trends and how the overall market is performing. He also has developed a keen intuition on how stocks might vary over time. In the past, he has been right 70% times.\n",
        "\n",
        "- **Employee of a competitor:** This person knows the internal functionality of the competitor firms and is aware of certain changes which are yet to be brought. He lacks a sight of the company in focus and the external factors which can relate the growth of competitor with the company of subject. In the past, he has been right 60% of times.\n",
        "\n",
        "- **Market Research team in the same segment:** This team analyzes the customer preference of company XYZ’s product over others and how is this changing with time. Because he deals with customer side, he is unaware of the changes company XYZ will bring because of alignment to its own goals. In the past, they have been right 75% of times.\n",
        "\n",
        "- **Social Media Expert:** This person can help us understand how has company XYZ has positioned its products in the market. And how are the sentiment of customers changing over time towards the company? He is unaware of any kind of details beyond digital marketing. In the past, he has been right 65% of times.\n",
        "\n",
        "Given the broad spectrum of access you have, you can probably combine all the information and make an informed decision.\n",
        "\n",
        "In a scenario when all the 6 experts/teams verify that it’s a good decision(assuming all the predictions are independent of each other), you will get a combined accuracy rate of \n",
        "$$ 1 - (30\\% \\times 25\\% \\times 30\\% \\times 40\\% \\times 25\\% \\times 35\\%) = 1 - 0.07875 = 99.92125\\% $$\n",
        "\n",
        "The assumption used here that all the predictions are completely independent is slightly extreme as they are expected to be correlated. However, you can see how we can be so sure by combining various forecasts together.\n",
        "\n",
        "Well, Ensemble learning is no different.\n",
        "\n",
        "An ensemble is the art of combining a diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions collectively will be termed as Ensemble learning.\n",
        "\n",
        "Moreover, Ensemble-based models can be incorporated in both of the two scenarios, i.e., when data is of large volume and when data is too little.\n",
        "\n",
        "Let’s now understand how do you actually get different set of machine learning models. Models can be different from each other for a variety of reasons:\n",
        "\n",
        "- There can be difference in the population of data.\n",
        "- There can be a different modeling technique used.\n",
        "- There can be a different hypothesis.\n",
        "\n",
        "Imagine that you are playing trivial pursuit. When you play alone, there might be some topics you are good at, and some that you know next to nothing about. If we want to maximize our trivial pursuit score, we need to build a team to cover all topics. This is the basic idea of an ensemble: combining predictions from several models averages out idiosyncratic errors and yield better overall predictions.\n",
        "\n",
        "The following picture shows an example schematics of an ensemble.\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535485453/Image1_Ensemble_o3hma0.png' />\n",
        "\n",
        "[Image Source](http://ml-ensemble.com/)\n",
        "\n",
        "In the picture above, An input array $X$ is fed through two preprocessing pipelines and then to a set of base learners $f^(i)$. The ensemble combines all base learner predictions into a final prediction array $P$.\n",
        "\n",
        "Now, the important question is how to combine predictions. In the trivial pursuit example, it is easy to imagine that team members might make their case and majority voting decides which to pick. Machine learning is remarkably similar in classification problems: taking the **most common** class label prediction is equivalent to a majority voting rule. But there are many other ways to combine predictions, and more generally you can use a model to learn how to combine predictions best.\n",
        "\n",
        "The following diagram presents a basic Ensemble structure:\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535485453/Image2_Ensemble_cndjda.png'/>\n",
        "\n",
        "[Image source](https://www.dataquest.io/blog/content/images/2018/01/ensemble_network.png)\n",
        "\n",
        "Here, Data is fed to a set of models, and a meta-learner combine model predictions.\n",
        "\n",
        "## Model error and reducing this error with Ensembles:\n",
        "The error emerging from any machine model can be broken down into three components mathematically. Following are these component:\n",
        "\n",
        "\n",
        "$$ Bias + Variance + Irreducible Error $$\n",
        "\n",
        "Why is this important in the current context? To understand what goes on behind an ensemble model, you need first to know what causes an error in the model. You will briefly get introduced to these errors.\n",
        "\n",
        "Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have an under-performing model which keeps on missing essential trends.\n",
        "\n",
        "Variance on the other side quantifies how are the prediction made on the same observation different from each other. A high variance model will over-fit on your training population and perform poorly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value, and blue dots are predictions):\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535483738/Image3_Ensemble_jmpiza.png' />\n",
        "\n",
        "[Image source](https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)\n",
        "\n",
        "Typically, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens until a particular point. As you continue to make your model more complex, you end up over-fitting your model, and hence your model will start suffering from the high variance.\n",
        "\n",
        "Now that you are familiar with the basics of Ensemble learning let's look at different Ensemble learning techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV1T2v9tyVYN",
        "colab_type": "text"
      },
      "source": [
        "## Different types of Ensemble learning methods:\n",
        "Although there are several types of Ensemble learning methods, the following three are the most-used ones in the industry.\n",
        "\n",
        "### Bagging based Ensemble learning:\n",
        "Bagging is one of the Ensemble construction techniques which is also known as Bootstrap Aggregation. Bootstrap establishes the foundation of Bagging technique. Bootstrap is a sampling technique in which we select “n” observations out of a population of “n” observations. But the selection is entirely random, i.e., each observation can be chosen from the original population so that each observation is equally likely to be selected in each iteration of the bootstrapping process. After the bootstrapped samples are formed, separate models are trained with the bootstrapped samples. In real experiments, the bootstrapped samples are drawn from the training set, and the sub-models are tested using the testing set. The final output prediction is combined across the projections of all the sub-models.\n",
        "\n",
        "The following infographic gives a brief idea of Bagging:\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535483738/Image4_Ensemble_prkbyo.png'/>\n",
        "\n",
        "[Image source](https://www.analyticsvidhya.com/wp-content/uploads/2015/07/bagging.png)\n",
        "\n",
        "### Boosting-based Ensemble learning:\n",
        "Boosting is a form of sequential learning technique. The algorithm works by training a model with the entire training set, and subsequent models are constructed by fitting the residual error values of the initial model. In this way, Boosting attempts to give higher weight to those observations that were poorly estimated by the previous model. Once the sequence of the models are created the predictions made by models are weighted by their accuracy scores and the results are combined to create a final estimation. Models that are typically used in Boosting technique are XGBoost (Extreme Gradient Boosting), GBM (Gradient Boosting Machine), ADABoost (Adaptive Boosting), etc.\n",
        "\n",
        "### Voting based Ensemble learning:\n",
        "Voting is one of the most straightforward Ensemble learning techniques in which predictions from multiple models are combined. The method starts with creating two or more separate models with the same dataset. Then a Voting based Ensemble model can be used to wrap the previous models and aggregate the predictions of those models. After the Voting based Ensemble model is constructed, it can be used to make a prediction on new data. The predictions made by the sub-models can be assigned weights. Stacked aggregation is a technique which can be used to learn how to weigh these predictions in the best possible way.\n",
        "\n",
        "The following infographic best describes Voting-based Ensembles:\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535483738/Image5_Ensemble_jchh9g.png' />\n",
        "\n",
        "[Image source](https://www.analyticsvidhya.com/wp-content/uploads/2015/07/stacking-297x300.png)\n",
        "\n",
        "Well, the time has come when you apply these concepts to strengthen your intuition and confidence. Let's do it in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZIuPHbyVYN",
        "colab_type": "text"
      },
      "source": [
        "## A case study in Python\n",
        "The dataset you are going to be using for this case study is popularly known as the Wisconsin Breast Cancer dataset. The task related to it is Classification.\n",
        "\n",
        "The dataset contains a total number of `10` features labeled in either benign or malignant classes. The features have `699` instances out of which `16` feature values are missing. The dataset only contains numeric values.\n",
        "\n",
        "You will implement the Ensembles using the mighty `scikit-learn` library.\n",
        "\n",
        "Let's first import all the Python dependencies you will be needing for this case study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvy7aIOCyVYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from sklearn.preprocessing import Imputer\n",
        "from sklearn.impute import SimpleImputer \n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8vnin0KyVYR",
        "colab_type": "text"
      },
      "source": [
        "Let's load the dataset in a DataFrame object. Since the data has no labels, we need to assemble labels to the data values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNMoSzINyVYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
        "       'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli',\n",
        "       'Mitoses', 'Class']\n",
        "data_url = 'https://raw.githubusercontent.com/DrJieTao/BA545-test/master/BA545-SP2019/data/cancer.csv?token=AES5WFLD67DBRWRBASZDJIK6SIRJA'\n",
        "data = pd.read_csv(data_url, names=cols)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F62-MGNyVYU",
        "colab_type": "text"
      },
      "source": [
        "The column `Sample code number` is just an indicator and it's of no use in the modeling. So, let's drop it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwMWqutmyVYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(['Sample code number'],axis = 1, inplace = True)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t2sG4otyVYW",
        "colab_type": "text"
      },
      "source": [
        "You can see that the column is dropped now. Let's get some statistics about the data using Panda's `describe()` and `info()` functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhgOksLQyVYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ5cg2f8yVYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jdQ6CFVyVYb",
        "colab_type": "text"
      },
      "source": [
        "As mentioned earlier, the dataset contains missing values. The column named `Bare Nuclei` contains them. Let's verify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8LhRbWByVYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.loc[data['Bare Nuclei'] == '?']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFLLpx2VyVYe",
        "colab_type": "text"
      },
      "source": [
        "You can spot some \"?\"s in it, right? Well, these are your missing values, and you will be imputing them with Mean Imputation. But first, you will replace those \"?\"s with `0`'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTqhImM0yVYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.replace('?',np.nan, inplace=True)\n",
        "data.loc[data['Bare Nuclei'] == '?']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvVdj9sqyVYg",
        "colab_type": "text"
      },
      "source": [
        "The blank df above proves that the \"?\"s are replaced with `0`'s now. Let's do the missing value treatment now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjXMYteIyVYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the DataFrame object into NumPy array otherwise you will not be able to impute\n",
        "values = data.values\n",
        "\n",
        "# Now impute it\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "#imputer = Imputer()\n",
        "imputedData = imputer.fit_transform(values)\n",
        "imputedData[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfnEYJ5yVYi",
        "colab_type": "text"
      },
      "source": [
        "Now if you take a look at the dataset itself, you will see that all the ranges of the features of the dataset are not the same. This may cause a problem. A small change in a feature might not affect the other. To address this problem, you will normalize the ranges of the features to a uniform range, in this case, `[0,1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9t7nhCtyVYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "normalizedData = scaler.fit_transform(imputedData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sA5c9fhyVYl",
        "colab_type": "text"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "You have performed all the preprocessing that was required in order to perform your Ensembling experiments.\n",
        "\n",
        "You will start with Bagging based Ensembling. In this case, you will use a Bagged Decision Tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIK3WZ_PyVYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bagged Decision Trees for Classification - necessary dependencies\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKHawpqqyVYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Segregate the features from the labels\n",
        "X = normalizedData[:,0:9]\n",
        "Y = normalizedData[:,9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gSJZ1AEyVYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
        "cart = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9WPWTE_6P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_selection.cross_val_score(cart, X, Y, cv=kfold).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZlTNH2yVYt",
        "colab_type": "text"
      },
      "source": [
        "Let's see what you did in the above cell.\n",
        "\n",
        "First, you initialized a 10-fold cross-validation fold. After that, you instantiated a Decision Tree Classifier with 100 trees and wrapped it in a **Bagging-based Ensemble**. Then you evaluated your model.\n",
        "\n",
        "You model performed pretty well. It yielded an accuracy of ~ 96%.\n",
        "\n",
        "Brilliant! Let's implement the other ones.\n",
        "\n",
        "(*If you want a quick refresher on cross-validation then this is the [video](https://www.youtube.com/watch?v=CRqLeHpACVI) to go for.*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy160wIuyVYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AdaBoost Classification\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "seed = 7\n",
        "num_trees = 70\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WediKRyVYv",
        "colab_type": "text"
      },
      "source": [
        "In this case, you did an AdaBoost classification (with 70 trees) which is based on **Boosting type of Ensembling**. The model gave you an accuracy of ~ 96% for 10-fold cross-validation.\n",
        "\n",
        "Finally, it's time for you to implement the **Voting-based Ensemble** technique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKAvQa9byVYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Voting Ensemble for Classification\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "# create the sub models\n",
        "estimators = []\n",
        "model1 = LogisticRegression()\n",
        "estimators.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "estimators.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "estimators.append(('svm', model3))\n",
        "# create the ensemble model\n",
        "ensemble = VotingClassifier(estimators)\n",
        "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOdRmpLbyVYx",
        "colab_type": "text"
      },
      "source": [
        "You implemented a Voting based Ensemble model where you took Logistic Regression, Decision Tree and Support Vector Machine for voting purpose. The model performed the best so far with an accuracy of 96.14% for 10-fold cross-validation.\n",
        "\n",
        "Now, let's get you familiarized with some common pitfalls of Ensemble learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ceewe2sSyVYy",
        "colab_type": "text"
      },
      "source": [
        "## Pitfalls of Ensemble learning\n",
        "In general, it is not true that it will **always** perform better. There are several ensemble methods, each with its own advantages/weaknesses. Which one to use and then depends on the problem at hand.\n",
        "\n",
        "For example, if you have models with high variance (they **fit better** with your training data), then you are likely to benefit from using **bagging**. If you have **biased** models, it is better to combine them with **Boosting**. There are also different strategies to form ensembles. The topic is just too broad to cover it in one answer.\n",
        "\n",
        "But the point is: **if you use the wrong ensemble method for your setting, you are not going to do better**. For example, using Bagging with a biased model is not going to help.\n",
        "\n",
        "Also, if you need to work in a **probabilistic** setting, ensemble methods may not work either. It is known that Boosting (in its most popular forms like AdaBoost) delivers poor probability estimates. That is, if you would like to have a model that allows you to reason about your data, not only classification, you might be better off with a graphical model.\n",
        "\n",
        "So, in this post, you got introduced to Ensemble learning technique. You covered its basics, how it improves your model's performance. You covered its three main types.\n",
        "\n",
        "Also, you implemented these three types in Python with the help of scikit-learn, and in this course of action, you gained a bit of knowledge about the necessary preprocessing steps.\n",
        "\n",
        "That's quite a feat! Well done! In this final section, I suggest some further undertakings on Ensembles which you might want to consider.\n",
        "\n",
        "## Take it further:\n",
        "- Try other Boosting-based Ensemble techniques viz. Gradient Boosting, XGBoost, etc.\n",
        "- Play with the different parameter settings that scikit-learn offers in Ensembles and then try to find why a particular setting performed well. This will make your understanding even stronger. [link](https://www.datacamp.com/community/tutorials/scikit-learn.org/stable/modules/ensemble.html)\n",
        "- Try Ensemble learning on a variety of datasets to understand where you should and where you should not apply Ensemble learning. For finding datasets Kaggle, UCI Repository, etc. are good places to search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9VmLx7xyVYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}